{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7_hx8SJJR4-"
      },
      "source": [
        "# Assignment 2 - Recurrent Neural Networks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AiWTVDf7cZ2"
      },
      "source": [
        "## Programming (Full points: 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6x4xiILLV-i"
      },
      "source": [
        "In this assignment, our goal is to use PyTorch to implement Recurrent Neural Networks (RNN) for sentiment analysis task. Sentiment analysis is to classify sentences (input) into certain sentiments (output labels), which includes positive, negative and neutral.\n",
        "\n",
        "We will use a benckmark dataset, SST, for this assignment.\n",
        "* we download the SST dataset from torchtext package, and do some preprocessing to build vocabulary and split the dataset into training/validation/test sets. You don't need to modify the code in this step.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#As per instruction, changing the torchtext version to 0.6.0.\n",
        "!pip install torchtext==0.6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKqdJ_N3v2hI",
        "outputId": "6f8585ca-2b70-4db2-9cfd-3c4caef27cc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Collecting sentencepiece (from torchtext==0.6.0)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (17.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.15.2\n",
            "    Uninstalling torchtext-0.15.2:\n",
            "      Successfully uninstalled torchtext-0.15.2\n",
            "Successfully installed sentencepiece-0.1.99 torchtext-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvppQHtZLV-i",
        "outputId": "f536b5e9-0b11-4d91-fdf6-b059395b83b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading trainDevTestTrees_PTB.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "trainDevTestTrees_PTB.zip: 100%|██████████| 790k/790k [00:02<00:00, 372kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extracting\n"
          ]
        }
      ],
      "source": [
        "#All required imports\n",
        "import copy\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "#Creating two variables for holding text and labels data.\n",
        "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
        "LABEL = data.LabelField()\n",
        "\n",
        "#Loading data splits from torchtext.datasets.\n",
        "train_data, val_data, test_data = datasets.SST.splits(TEXT, LABEL)\n",
        "\n",
        "#Building vocabulary dictionaries for Text and Label.\n",
        "TEXT.build_vocab(train_data)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "#Initializing the hyperparameters of the model.\n",
        "vocab_size = len(TEXT.vocab)\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "embedding_dim = 128\n",
        "hidden_dim = 128\n",
        "\n",
        "#Building the iterators.\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train_data, val_data, test_data),\n",
        "    batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ae5rC0GLV-j"
      },
      "source": [
        "* define the training and evaluation function in the cell below.\n",
        "### (25 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aoZb9uhLV-k"
      },
      "outputs": [],
      "source": [
        "#Defining the training model.\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    '''\n",
        "    This function is used to train the model.\n",
        "    '''\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        text, labels = batch.text, batch.label\n",
        "        predictions = model(text)\n",
        "        loss = criterion(predictions, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        total_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "    return total_loss / len(iterator), total_correct / len(iterator.dataset)\n",
        "\n",
        "#Defining the evaluation function.\n",
        "def evaluate(model, iterator, criterion):\n",
        "    '''\n",
        "    This function is used to evaluate the model trained.\n",
        "    '''\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text, labels = batch.text, batch.label\n",
        "            predictions = model(text)\n",
        "            loss = criterion(predictions, labels)\n",
        "            total_loss += loss.item()\n",
        "            total_correct += (predictions.argmax(1) == labels).sum().item()\n",
        "    return total_loss / len(iterator), total_correct / len(iterator.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRKuEJSrLV-k"
      },
      "source": [
        "* build a RNN model for sentiment analysis in the cell below.\n",
        "We have provided several hyperparameters we needed for building the model, including vocabulary size (vocab_size), the word embedding dimension (embedding_dim), the hidden layer dimension (hidden_dim), the number of layers (num_layers) and the number of sentence labels (label_size). Please fill in the missing codes, and implement a RNN model.\n",
        "### (40 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-i8alf6LV-l"
      },
      "outputs": [],
      "source": [
        "#Defining the RNNClassifier model for sentiment classification task.\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, num_layers=1):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, label_size)\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        last_output = output[:, -1, :]\n",
        "        logits = self.fc(last_output)\n",
        "        return logits\n",
        "\n",
        "#Creating the model with the hyperparameters provided and ADAM optimizer and crossentropy loss function.\n",
        "num_layers = 1\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, num_layers)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhuxVTQPLV-m"
      },
      "source": [
        "* train the model and compute the accuracy in the cell below.\n",
        "### (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swNl-B-aLV-m",
        "outputId": "ea590a9d-8eb1-4eff-a1c5-b63089a159fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 1.054 | Train Acc: 41.60%\n",
            "\tValid Loss: 1.055 | Valid Acc: 39.51%\n",
            "Epoch: 02\n",
            "\tTrain Loss: 1.047 | Train Acc: 42.21%\n",
            "\tValid Loss: 1.066 | Valid Acc: 40.24%\n",
            "Epoch: 03\n",
            "\tTrain Loss: 1.039 | Train Acc: 42.65%\n",
            "\tValid Loss: 1.165 | Valid Acc: 40.96%\n",
            "Epoch: 04\n",
            "\tTrain Loss: 1.003 | Train Acc: 49.65%\n",
            "\tValid Loss: 1.166 | Valid Acc: 47.77%\n",
            "Epoch: 05\n",
            "\tTrain Loss: 0.892 | Train Acc: 61.38%\n",
            "\tValid Loss: 1.048 | Valid Acc: 51.95%\n",
            "Epoch: 06\n",
            "\tTrain Loss: 0.746 | Train Acc: 68.88%\n",
            "\tValid Loss: 1.141 | Valid Acc: 53.86%\n",
            "Epoch: 07\n",
            "\tTrain Loss: 0.619 | Train Acc: 74.80%\n",
            "\tValid Loss: 1.174 | Valid Acc: 53.59%\n",
            "Epoch: 08\n",
            "\tTrain Loss: 0.520 | Train Acc: 79.65%\n",
            "\tValid Loss: 1.282 | Valid Acc: 53.68%\n",
            "Epoch: 09\n",
            "\tTrain Loss: 0.425 | Train Acc: 84.70%\n",
            "\tValid Loss: 1.330 | Valid Acc: 51.77%\n",
            "Epoch: 10\n",
            "\tTrain Loss: 0.359 | Train Acc: 87.52%\n",
            "\tValid Loss: 1.399 | Valid Acc: 53.50%\n",
            "Test Loss: 1.049 | Test Acc: 53.94%\n"
          ]
        }
      ],
      "source": [
        "#Training the model.\n",
        "N_EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    #Train and validate model.\n",
        "    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, val_iter, criterion)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\tValid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "    #Saving the best model.\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'rnn_model.pt')\n",
        "\n",
        "#Loading the best model.\n",
        "model.load_state_dict(torch.load('rnn_model.pt'))\n",
        "\n",
        "#Testing the model trained.\n",
        "test_loss, test_acc = evaluate(model, test_iter, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK1TpJL9LV-n"
      },
      "source": [
        "* try to train a model with better accuracy in the cell below. For example, you can use different optimizers such as SGD and Adam. You can also compare different hyperparameters and model size.\n",
        "### (15 points), to obtain FULL point in this problem, the accuracy needs to be higher than 70%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9NSeI4mLV-n",
        "outputId": "cc1d1e47-208e-4a4a-ff74-d1c6cadca250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 1.054 | Train Acc: 41.37%\n",
            "\tValid Loss: 1.063 | Valid Acc: 40.24%\n",
            "Epoch: 02\n",
            "\tTrain Loss: 1.047 | Train Acc: 42.02%\n",
            "\tValid Loss: 1.083 | Valid Acc: 38.78%\n",
            "Epoch: 03\n",
            "\tTrain Loss: 1.043 | Train Acc: 42.33%\n",
            "\tValid Loss: 1.128 | Valid Acc: 37.51%\n",
            "Epoch: 04\n",
            "\tTrain Loss: 1.035 | Train Acc: 42.74%\n",
            "\tValid Loss: 1.165 | Valid Acc: 39.69%\n",
            "Epoch: 05\n",
            "\tTrain Loss: 1.027 | Train Acc: 43.97%\n",
            "\tValid Loss: 1.199 | Valid Acc: 41.60%\n",
            "Epoch: 06\n",
            "\tTrain Loss: 1.005 | Train Acc: 48.60%\n",
            "\tValid Loss: 1.215 | Valid Acc: 49.23%\n",
            "Epoch: 07\n",
            "\tTrain Loss: 0.882 | Train Acc: 62.31%\n",
            "\tValid Loss: 1.068 | Valid Acc: 54.86%\n",
            "Epoch: 08\n",
            "\tTrain Loss: 0.675 | Train Acc: 72.93%\n",
            "\tValid Loss: 1.093 | Valid Acc: 57.86%\n",
            "Epoch: 09\n",
            "\tTrain Loss: 0.509 | Train Acc: 79.17%\n",
            "\tValid Loss: 1.207 | Valid Acc: 56.49%\n",
            "Epoch: 10\n",
            "\tTrain Loss: 0.382 | Train Acc: 85.65%\n",
            "\tValid Loss: 1.287 | Valid Acc: 58.95%\n",
            "Epoch: 11\n",
            "\tTrain Loss: 0.279 | Train Acc: 90.70%\n",
            "\tValid Loss: 1.399 | Valid Acc: 57.95%\n",
            "Epoch: 12\n",
            "\tTrain Loss: 0.208 | Train Acc: 93.47%\n",
            "\tValid Loss: 1.418 | Valid Acc: 57.67%\n",
            "Epoch: 13\n",
            "\tTrain Loss: 0.155 | Train Acc: 95.22%\n",
            "\tValid Loss: 1.472 | Valid Acc: 59.04%\n",
            "Epoch: 14\n",
            "\tTrain Loss: 0.115 | Train Acc: 96.52%\n",
            "\tValid Loss: 1.589 | Valid Acc: 57.22%\n",
            "Epoch: 15\n",
            "\tTrain Loss: 0.079 | Train Acc: 97.66%\n",
            "\tValid Loss: 1.668 | Valid Acc: 58.13%\n",
            "Epoch: 16\n",
            "\tTrain Loss: 0.065 | Train Acc: 98.23%\n",
            "\tValid Loss: 1.728 | Valid Acc: 57.86%\n",
            "Epoch: 17\n",
            "\tTrain Loss: 0.072 | Train Acc: 97.89%\n",
            "\tValid Loss: 1.757 | Valid Acc: 56.77%\n",
            "Epoch: 18\n",
            "\tTrain Loss: 0.060 | Train Acc: 98.37%\n",
            "\tValid Loss: 1.846 | Valid Acc: 58.40%\n",
            "Epoch: 19\n",
            "\tTrain Loss: 0.057 | Train Acc: 98.40%\n",
            "\tValid Loss: 1.938 | Valid Acc: 57.40%\n",
            "Epoch: 20\n",
            "\tTrain Loss: 0.046 | Train Acc: 98.67%\n",
            "\tValid Loss: 1.912 | Valid Acc: 57.95%\n",
            "Test Loss: 1.044 | Test Acc: 41.40%\n"
          ]
        }
      ],
      "source": [
        "#Attempt1:\n",
        "# 1) Since, the model is overfitting, I tried to use the technique of dropout.\n",
        "# 2) Increased the number of epochs.\n",
        "# 3) Provided the learning rate of 0.001.\n",
        "# 4) Added a weight_decay factor of 1e-5 so as to do the L2 regularization.\n",
        "#Updating the definition of the RNNClassifier model.\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, num_layers=1):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, label_size)\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        last_output = output[:, -1, :]\n",
        "        logits = self.fc(self.dropout(last_output))\n",
        "        return logits\n",
        "\n",
        "#Re-creating the model.\n",
        "num_layers = 1\n",
        "#Initializing the hyperparameters of the model.\n",
        "vocab_size = len(TEXT.vocab)\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "embedding_dim = 256\n",
        "hidden_dim = 256\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, num_layers)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Training the model.\n",
        "N_EPOCHS = 20\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    #Train and validate the model.\n",
        "    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, val_iter, criterion)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\tValid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "    #Saving the best model.\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'rnn_model.pt')\n",
        "\n",
        "#Loading the best model.\n",
        "model.load_state_dict(torch.load('rnn_model.pt'))\n",
        "\n",
        "#Testing the model.\n",
        "test_loss, test_acc = evaluate(model, test_iter, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Attempt2:\n",
        "# 1) Since, the model is overfitting, I tried to use the technique of dropout.\n",
        "# 2) Increased the number of epochs.\n",
        "# 3) Used a different optimizer Stochastic Gradient Descent with learning rate of 0.01 and momentum of 0.5.\n",
        "#Re-creating the model.\n",
        "num_layers = 1\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, num_layers)\n",
        "\n",
        "#Trying another optimizer (SGD with momentum).\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Training the model.\n",
        "for epoch in range(N_EPOCHS):\n",
        "    #Train and validate the model.\n",
        "    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, val_iter, criterion)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\tValid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "    #Saving the best model.\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'rnn_model.pt')\n",
        "\n",
        "#Loading the best model.\n",
        "model.load_state_dict(torch.load('rnn_model.pt'))\n",
        "\n",
        "#Testing the model.\n",
        "test_loss, test_acc = evaluate(model, test_iter, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7VdAs6hZYNn",
        "outputId": "3d406eb1-b71d-4ef4-f5ec-5f55311110b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 1.061 | Train Acc: 40.65%\n",
            "\tValid Loss: 1.055 | Valid Acc: 40.60%\n",
            "Epoch: 02\n",
            "\tTrain Loss: 1.047 | Train Acc: 42.33%\n",
            "\tValid Loss: 1.055 | Valid Acc: 40.51%\n",
            "Epoch: 03\n",
            "\tTrain Loss: 1.047 | Train Acc: 42.18%\n",
            "\tValid Loss: 1.054 | Valid Acc: 40.69%\n",
            "Epoch: 04\n",
            "\tTrain Loss: 1.047 | Train Acc: 42.15%\n",
            "\tValid Loss: 1.055 | Valid Acc: 40.51%\n",
            "Epoch: 05\n",
            "\tTrain Loss: 1.047 | Train Acc: 42.31%\n",
            "\tValid Loss: 1.054 | Valid Acc: 40.60%\n",
            "Epoch: 06\n",
            "\tTrain Loss: 1.047 | Train Acc: 42.35%\n",
            "\tValid Loss: 1.056 | Valid Acc: 40.51%\n",
            "Epoch: 07\n",
            "\tTrain Loss: 1.047 | Train Acc: 42.42%\n",
            "\tValid Loss: 1.057 | Valid Acc: 40.60%\n",
            "Epoch: 08\n",
            "\tTrain Loss: 1.047 | Train Acc: 42.30%\n",
            "\tValid Loss: 1.056 | Valid Acc: 40.60%\n",
            "Epoch: 09\n",
            "\tTrain Loss: 1.047 | Train Acc: 42.25%\n",
            "\tValid Loss: 1.054 | Valid Acc: 40.60%\n",
            "Epoch: 10\n",
            "\tTrain Loss: 1.046 | Train Acc: 42.38%\n",
            "\tValid Loss: 1.056 | Valid Acc: 40.51%\n",
            "Epoch: 11\n",
            "\tTrain Loss: 1.046 | Train Acc: 42.29%\n",
            "\tValid Loss: 1.053 | Valid Acc: 40.60%\n",
            "Epoch: 12\n",
            "\tTrain Loss: 1.046 | Train Acc: 42.37%\n",
            "\tValid Loss: 1.054 | Valid Acc: 40.69%\n",
            "Epoch: 13\n",
            "\tTrain Loss: 1.047 | Train Acc: 42.31%\n",
            "\tValid Loss: 1.054 | Valid Acc: 40.69%\n",
            "Epoch: 14\n",
            "\tTrain Loss: 1.046 | Train Acc: 42.36%\n",
            "\tValid Loss: 1.056 | Valid Acc: 40.60%\n",
            "Epoch: 15\n",
            "\tTrain Loss: 1.047 | Train Acc: 42.32%\n",
            "\tValid Loss: 1.056 | Valid Acc: 40.51%\n",
            "Epoch: 16\n",
            "\tTrain Loss: 1.047 | Train Acc: 42.31%\n",
            "\tValid Loss: 1.055 | Valid Acc: 40.24%\n",
            "Epoch: 17\n",
            "\tTrain Loss: 1.046 | Train Acc: 42.36%\n",
            "\tValid Loss: 1.056 | Valid Acc: 40.24%\n",
            "Epoch: 18\n",
            "\tTrain Loss: 1.046 | Train Acc: 42.39%\n",
            "\tValid Loss: 1.054 | Valid Acc: 40.33%\n",
            "Epoch: 19\n",
            "\tTrain Loss: 1.046 | Train Acc: 42.18%\n",
            "\tValid Loss: 1.054 | Valid Acc: 40.51%\n",
            "Epoch: 20\n",
            "\tTrain Loss: 1.046 | Train Acc: 42.26%\n",
            "\tValid Loss: 1.052 | Valid Acc: 40.60%\n",
            "Test Loss: 1.033 | Test Acc: 41.49%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Attempt3:\n",
        "# 1) Since, the model is overfitting, I tried to use the technique of bidrectional RNNs.\n",
        "# 2) Increased the number of epochs.\n",
        "# 3) Provided the learning rate of 0.001.\n",
        "# 4) Added a weight_decay factor of 1e-5 so as to do the L2 regularization.\n",
        "#Updating the definition of the RNNClassifier model.\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, num_layers=1, bidirectional=True, dropout_prob=0.3):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
        "                           batch_first=True, bidirectional=bidirectional, dropout=dropout_prob)\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, label_size)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        last_output = output[:, -1, :]\n",
        "        logits = self.fc(last_output)\n",
        "        return logits\n",
        "\n",
        "#Re-creating the model with change in hyperparameter value.\n",
        "vocab_size = len(TEXT.vocab)\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "embedding_dim = 256\n",
        "hidden_dim = 256\n",
        "num_layers = 1\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, num_layers)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Training the model.\n",
        "N_EPOCHS = 30\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    #Train and evaluate the model.\n",
        "    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, val_iter, criterion)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\tValid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "    #Saving the best model.\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'rnn_model.pt')\n",
        "\n",
        "#Loading the best model.\n",
        "model.load_state_dict(torch.load('rnn_model.pt'))\n",
        "\n",
        "#Testing the model.\n",
        "test_loss, test_acc = evaluate(model, test_iter, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        },
        "id": "FboBI-e4h-Da",
        "outputId": "4ba34b87-adc4-4cd6-a32d-f2421f5b50fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 1.051 | Train Acc: 41.43%\n",
            "\tValid Loss: 1.071 | Valid Acc: 40.51%\n",
            "Epoch: 02\n",
            "\tTrain Loss: 1.047 | Train Acc: 41.90%\n",
            "\tValid Loss: 1.067 | Valid Acc: 37.78%\n",
            "Epoch: 03\n",
            "\tTrain Loss: 1.038 | Train Acc: 43.13%\n",
            "\tValid Loss: 1.081 | Valid Acc: 40.87%\n",
            "Epoch: 04\n",
            "\tTrain Loss: 1.025 | Train Acc: 46.79%\n",
            "\tValid Loss: 1.094 | Valid Acc: 46.87%\n",
            "Epoch: 05\n",
            "\tTrain Loss: 1.013 | Train Acc: 50.14%\n",
            "\tValid Loss: 1.095 | Valid Acc: 47.32%\n",
            "Epoch: 06\n",
            "\tTrain Loss: 1.019 | Train Acc: 47.45%\n",
            "\tValid Loss: 1.068 | Valid Acc: 40.05%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-7479ba0cde4b>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m#Train and evaluate the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-91e366ef5982>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtotal_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 state_steps)\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    282\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Attempt4:\n",
        "# 1) Since, the model is overfitting, I tried to use the technique of bidrectional RNNs.\n",
        "# 2) Increased the number of epochs.\n",
        "# 3) Provided the learning rate of 0.001.\n",
        "# 4) Added a weight_decay factor of 1e-5 so as to do the L2 regularization.\n",
        "#Updating the definition of the RNNClassifier model.\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, num_layers=1, bidirectional=True):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
        "                           batch_first=True, bidirectional=bidirectional)\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, label_size)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        last_output = output[:, -1, :]\n",
        "        logits = self.fc(last_output)\n",
        "        return logits\n",
        "\n",
        "#Re-creating the model.\n",
        "num_layers = 1\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, num_layers)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Training the model\n",
        "N_EPOCHS = 30\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    #Train and validate the model.\n",
        "    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, val_iter, criterion)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\tValid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "    #Saving the best model.\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'rnn_model.pt')\n",
        "\n",
        "#Loading the best model.\n",
        "model.load_state_dict(torch.load('rnn_model.pt'))\n",
        "\n",
        "#Testing the model.\n",
        "test_loss, test_acc = evaluate(model, test_iter, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "id": "G0_Q6azDkOeA",
        "outputId": "09f5b1dc-b07b-42fe-d73b-7f23ddee94f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 1.053 | Train Acc: 41.83%\n",
            "\tValid Loss: 1.062 | Valid Acc: 40.33%\n",
            "Epoch: 02\n",
            "\tTrain Loss: 1.046 | Train Acc: 42.28%\n",
            "\tValid Loss: 1.104 | Valid Acc: 40.42%\n",
            "Epoch: 03\n",
            "\tTrain Loss: 1.041 | Train Acc: 42.29%\n",
            "\tValid Loss: 1.118 | Valid Acc: 43.87%\n",
            "Epoch: 04\n",
            "\tTrain Loss: 1.032 | Train Acc: 43.42%\n",
            "\tValid Loss: 1.167 | Valid Acc: 42.33%\n",
            "Epoch: 05\n",
            "\tTrain Loss: 1.008 | Train Acc: 49.91%\n",
            "\tValid Loss: 1.092 | Valid Acc: 48.23%\n",
            "Epoch: 06\n",
            "\tTrain Loss: 0.991 | Train Acc: 53.08%\n",
            "\tValid Loss: 1.107 | Valid Acc: 51.68%\n",
            "Epoch: 07\n",
            "\tTrain Loss: 0.944 | Train Acc: 58.49%\n",
            "\tValid Loss: 1.145 | Valid Acc: 50.86%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-8cafacaf01af>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m#Train and validate the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-91e366ef5982>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-e0z8FTGl6S8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}