# CS577: Deep Learning (Fall 2023, Prof. Yan Yan)
Illinois Institute of Technology

This repository contains my learning journey in the subject of Deep Learning. The course covered a variety of topics to equip me with the fundamental skills in advanced deep learning methodologies. Below is a summary of the key concepts and techniques learned:

## Table of Contents

- Introduction to Deep Learning
- Convolutional Neural Networks (CNN)
- Recurrent Neural Networks (RNN)
- Deep Neural Networks (DNN)
- Transfer Learning
- Transformers
- Generative Adversarial Networks (GANs)

### Introduction to Deep Learning

The course began with an introduction to the field of deep learning, providing insights into its applications and importance in various domains such as computer vision, natural language processing, and more.

### Convolutional Neural Networks (CNN)

Studied the architecture and applications of Convolutional Neural Networks (CNNs) in image recognition and computer vision. Topics included convolutional layers, pooling layers, activation functions, and popular CNN architectures like VGG, ResNet, and Inception.

### Recurrent Neural Networks (RNN)

Explored the structure and use cases of Recurrent Neural Networks (RNNs) for sequential data analysis. Covered the basics of RNNs, Long Short-Term Memory (LSTM) networks, and Gated Recurrent Units (GRUs), focusing on their applications in natural language processing and time series prediction.

### Deep Neural Networks (DNN)

Gained an understanding of deep neural networks (DNNs), including their architecture, training processes, and applications. Topics included feedforward neural networks, backpropagation, gradient descent, and the importance of activation functions.

### Transfer Learning

Investigated transfer learning techniques to leverage pre-trained models for new tasks. Covered the benefits of transfer learning, common pre-trained models (e.g., VGG, ResNet), and practical applications in various domains.

### Transformers

Dived into the architecture and applications of Transformers, a breakthrough in natural language processing. Topics included self-attention mechanisms, positional encoding, and popular models like BERT, GPT, and Transformer-XL.

### Generative Adversarial Networks (GANs)

Explored the concept and structure of Generative Adversarial Networks (GANs). Studied how GANs consist of generator and discriminator networks and their applications in generating realistic images, data augmentation, and more.

---

This repository serves as a comprehensive record of my journey through the Deep Learning course, showcasing the skills and knowledge gained in various deep learning methodologies. Feel free to explore the code and documentation for a detailed insight into each topic.
